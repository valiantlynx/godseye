{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_working_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hokey datasett:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def restructure_videos(input_path):\n",
    "    # Define the target directories for Fight and NonFight\n",
    "    fight_dir = os.path.join(input_path, 'Fight')\n",
    "    nonfight_dir = os.path.join(input_path, 'NonFight')\n",
    "\n",
    "    # Create the directories if they do not exist\n",
    "    if not os.path.exists(fight_dir):\n",
    "        os.makedirs(fight_dir)\n",
    "\n",
    "    if not os.path.exists(nonfight_dir):\n",
    "        os.makedirs(nonfight_dir)\n",
    "\n",
    "    # Iterate over files in the input directory\n",
    "    for file_name in os.listdir(input_path):\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "\n",
    "        # Check if the file name starts with \"fi\" (Fight) or \"no\" (NonFight)\n",
    "        if file_name.lower().startswith('fi'):\n",
    "            # Move to 'fight' directory\n",
    "            shutil.move(file_path, os.path.join(fight_dir, file_name))\n",
    "            print(f\"Moved {file_name} to 'fight' directory\")\n",
    "        elif file_name.lower().startswith('no'):\n",
    "            # Move to 'nonfight' directory\n",
    "            shutil.move(file_path, os.path.join(nonfight_dir, file_name))\n",
    "            print(f\"Moved {file_name} to 'nonfight' directory\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_name} (does not match 'fi' or 'no')\")\n",
    "\n",
    "# Example usage\n",
    "# input_path = os.path.join(current_working_directory, \"archive\", \"hockey\")\n",
    "# restructure_videos(input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airtby datasett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "def restructure_dataset(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Restructure dataset to match the desired format with unique IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): Path to the input dataset directory.\n",
    "        output_dir (str): Path to the output dataset directory.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create subdirectories for \"Fight\" and \"NonFight\"\n",
    "    fight_dir = os.path.join(output_dir, \"Fight\")\n",
    "    non_fight_dir = os.path.join(output_dir, \"NonFight\")\n",
    "    os.makedirs(fight_dir, exist_ok=True)\n",
    "    os.makedirs(non_fight_dir, exist_ok=True)\n",
    "\n",
    "    # Process \"violent\" and \"non-violent\" directories\n",
    "    for category in [\"violent\", \"non-violent\"]:\n",
    "        category_path = os.path.join(input_dir, category)\n",
    "\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"Category directory not found: {category_path}\")\n",
    "            continue\n",
    "\n",
    "        # Determine target directory\n",
    "        target_dir = fight_dir if category == \"violent\" else non_fight_dir\n",
    "\n",
    "        # Process subdirectories (e.g., \"cam1\", \"cam2\")\n",
    "        for subdir in os.listdir(category_path):\n",
    "            subdir_path = os.path.join(category_path, subdir)\n",
    "\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                continue\n",
    "\n",
    "            # Process video files in each subdirectory\n",
    "            for filename in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, filename)\n",
    "\n",
    "                if not os.path.isfile(file_path):\n",
    "                    continue\n",
    "\n",
    "                # Generate a unique ID for each video\n",
    "                unique_id = str(uuid.uuid4()) + os.path.splitext(filename)[1]\n",
    "\n",
    "                # Copy the file to the target directory\n",
    "                target_file_path = os.path.join(target_dir, unique_id)\n",
    "                shutil.copy(file_path, target_file_path)\n",
    "\n",
    "    print(f\"Dataset restructured successfully to: {output_dir}\")\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# input_dataset_path = os.path.join(current_working_directory, \"archive\", \"violence-detection-dataset\")\n",
    "# output_dataset_path = os.path.join(current_working_directory, \"archive\", \"airtby\")\n",
    "# restructure_dataset(input_dataset_path, output_dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split it to train and val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(input_dir, output_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split dataset into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Path to the input dataset directory (with \"Fight\" and \"NonFight\").\n",
    "        output_dir (str): Path to the output dataset directory.\n",
    "        train_ratio (float): Ratio of data to use for training (default is 80%).\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"val\")\n",
    "    \n",
    "    for dir_path in [train_dir, val_dir]:\n",
    "        os.makedirs(os.path.join(dir_path, \"Fight\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, \"NonFight\"), exist_ok=True)\n",
    "\n",
    "    # Process each category (Fight and NonFight)\n",
    "    for label in [\"Fight\", \"NonFight\"]:\n",
    "        input_label_dir = os.path.join(input_dir, label)\n",
    "        train_label_dir = os.path.join(train_dir, label)\n",
    "        val_label_dir = os.path.join(val_dir, label)\n",
    "\n",
    "        # Get all video files in the category\n",
    "        videos = [f for f in os.listdir(input_label_dir) if os.path.isfile(os.path.join(input_label_dir, f))]\n",
    "        random.shuffle(videos)  # Shuffle to ensure randomness\n",
    "\n",
    "        # Calculate split point\n",
    "        split_index = int(len(videos) * train_ratio)\n",
    "\n",
    "        # Split videos into training and validation sets\n",
    "        train_videos = videos[:split_index]\n",
    "        val_videos = videos[split_index:]\n",
    "\n",
    "        # Copy files to the respective directories\n",
    "        for video in train_videos:\n",
    "            shutil.copy(os.path.join(input_label_dir, video), os.path.join(train_label_dir, video))\n",
    "\n",
    "        for video in val_videos:\n",
    "            shutil.copy(os.path.join(input_label_dir, video), os.path.join(val_label_dir, video))\n",
    "\n",
    "    print(f\"Dataset split successfully into training and validation sets at: {output_dir}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dataset_path = os.path.join(current_working_directory, \"archive\", \"airtby\")\n",
    "output_dataset_path = os.path.join(current_working_directory, \"archive\", \"airtby\")\n",
    "split_dataset(input_dataset_path, output_dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_dataset_into_train_val(input_dir, output_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split the dataset into train and val directories, maintaining structure.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the input dataset directory.\n",
    "        output_dir (str): Path to the output directory for the split dataset.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "    \"\"\"\n",
    "    # Categories (Fight and NonFight)\n",
    "    categories = [\"Fight\", \"NonFight\"]\n",
    "\n",
    "    for category in categories:\n",
    "        input_category_dir = os.path.join(input_dir, category)\n",
    "        if not os.path.exists(input_category_dir):\n",
    "            print(f\"Category directory {input_category_dir} does not exist. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect all subdirectories\n",
    "        video_folders = [f for f in os.listdir(input_category_dir) if os.path.isdir(os.path.join(input_category_dir, f))]\n",
    "        random.shuffle(video_folders)\n",
    "\n",
    "        # Split into train and val\n",
    "        split_index = int(len(video_folders) * train_ratio)\n",
    "        train_folders = video_folders[:split_index]\n",
    "        val_folders = video_folders[split_index:]\n",
    "\n",
    "        # Copy files to the new structure\n",
    "        for split, folders in [(\"train\", train_folders), (\"val\", val_folders)]:\n",
    "            split_category_dir = os.path.join(output_dir, split, category)\n",
    "            os.makedirs(split_category_dir, exist_ok=True)\n",
    "\n",
    "            for folder in tqdm(folders, desc=f\"Processing {category} for {split}\"):\n",
    "                src_folder = os.path.join(input_category_dir, folder)\n",
    "                dest_folder = os.path.join(split_category_dir, folder)\n",
    "                shutil.copytree(src_folder, dest_folder, dirs_exist_ok=True)\n",
    "\n",
    "input_dataset_dir = os.path.join(current_working_directory, \"archive\", \"Keypoints-hokey\")\n",
    "output_dataset_dir = os.path.join(current_working_directory, \"archive\", \"Keypoints-hokey\")\n",
    "split_dataset_into_train_val(input_dataset_dir, output_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasetts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging train/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-airtby: 100%|██████████| 184/184 [00:14<00:00, 12.82it/s]\n",
      "Merging train/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-hokey: 100%|██████████| 400/400 [00:17<00:00, 22.32it/s]\n",
      "Merging train/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-rwf-2000: 100%|██████████| 789/789 [00:46<00:00, 17.01it/s]\n",
      "Merging train/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-airtby: 100%|██████████| 96/96 [00:06<00:00, 13.98it/s]\n",
      "Merging train/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-hokey: 100%|██████████| 400/400 [00:18<00:00, 21.83it/s]\n",
      "Merging train/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-rwf-2000: 100%|██████████| 802/802 [00:52<00:00, 15.15it/s]\n",
      "Merging val/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-airtby: 100%|██████████| 46/46 [00:04<00:00, 10.02it/s]\n",
      "Merging val/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-hokey: 100%|██████████| 100/100 [00:04<00:00, 23.35it/s]\n",
      "Merging val/Fight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-rwf-2000: 100%|██████████| 200/200 [00:15<00:00, 12.58it/s]\n",
      "Merging val/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-airtby: 100%|██████████| 24/24 [00:02<00:00, 11.13it/s]\n",
      "Merging val/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-hokey: 100%|██████████| 100/100 [00:04<00:00, 24.38it/s]\n",
      "Merging val/NonFight from c:\\Users\\gorme\\projects\\godseye\\apps\\backend\\dataset_processing\\archive\\Keypoints-rwf-2000: 100%|██████████| 200/200 [00:12<00:00, 15.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_datasets(dataset_paths, output_dir):\n",
    "    \"\"\"\n",
    "    Merge multiple datasets into a single dataset structure.\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (list): List of dataset paths to merge.\n",
    "        output_dir (str): Path to the output unified dataset.\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        for category in [\"Fight\", \"NonFight\"]:\n",
    "            unified_dir = os.path.join(output_dir, split, category)\n",
    "            os.makedirs(unified_dir, exist_ok=True)\n",
    "\n",
    "            for dataset_path in dataset_paths:\n",
    "                source_dir = os.path.join(dataset_path, split, category)\n",
    "                if os.path.exists(source_dir):\n",
    "                    for video_folder in tqdm(\n",
    "                        os.listdir(source_dir),\n",
    "                        desc=f\"Merging {split}/{category} from {dataset_path}\",\n",
    "                    ):\n",
    "                        source_folder_path = os.path.join(source_dir, video_folder)\n",
    "                        dest_folder_path = os.path.join(unified_dir, video_folder)\n",
    "\n",
    "                        if os.path.isdir(source_folder_path):\n",
    "                            # Handle name conflicts by appending a unique suffix\n",
    "                            if os.path.exists(dest_folder_path):\n",
    "                                base_name = os.path.basename(video_folder)\n",
    "                                dest_folder_path = os.path.join(\n",
    "                                    unified_dir, f\"{base_name}_{os.urandom(4).hex()}\"\n",
    "                                )\n",
    "                            shutil.copytree(\n",
    "                                source_folder_path, dest_folder_path, dirs_exist_ok=True\n",
    "                            )\n",
    "                        else:\n",
    "                            print(f\"Skipping non-directory: {source_folder_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of dataset paths to merge\n",
    "    dataset_paths = [\n",
    "        os.path.join(current_working_directory, \"archive\", \"Keypoints-airtby\"),\n",
    "        os.path.join(current_working_directory, \"archive\", \"Keypoints-hokey\"),\n",
    "        os.path.join(current_working_directory, \"archive\", \"Keypoints-rwf-2000\"),\n",
    "    ]\n",
    "    # Unified output dataset directory\n",
    "    output_dataset_dir = os.path.join(current_working_directory, \"archive\", \"Keypoints-total\")\n",
    "    merge_datasets(dataset_paths, output_dataset_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
